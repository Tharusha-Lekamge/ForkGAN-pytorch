{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/\n",
      "  trainB/\n",
      "  trainA/\n",
      "  testA/\n",
      "  testB/\n"
     ]
    }
   ],
   "source": [
    "# print the folder structure of ./datasets folder\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def print_folder_structure(folder, indent=0):\n",
    "    for item in os.listdir(folder):\n",
    "        if item.startswith('.'):\n",
    "            continue\n",
    "        path = os.path.join(folder, item)\n",
    "        if os.path.isdir(path):\n",
    "            print(' ' * indent + item + '/')\n",
    "            print_folder_structure(path, indent + 2)\n",
    "        else:\n",
    "            #print(' ' * indent + item)\n",
    "            pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print_folder_structure('./datasets')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset folder has a file status.yaml that contains the status of the dataset\n",
    "# set the status to 'augmented' to indicate that the dataset is already augmented\n",
    "# set the status to 'original' to indicate that the dataset is the original dataset\n",
    "\n",
    "# function to set the status of the dataset\n",
    "def set_status(status):\n",
    "    with open('./datasets/dataset/status.yaml', 'w') as f:\n",
    "        f.write('status: ' + status + '\\n')\n",
    "\n",
    "# function to read the status of the dataset\n",
    "def read_status():\n",
    "    with open('./datasets/dataset/status.yaml', 'r') as f:\n",
    "        # if the file is empty, return 'original'\n",
    "        if not f.read().strip():\n",
    "            return 'original'\n",
    "        return f.read().strip().split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting dataset:   0%|          | 0/485 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167402/2883245858.py:40: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  augmented_img = augmented_img.resize(new_size, Image.ANTIALIAS)\n",
      "Augmenting dataset: 100%|██████████| 485/485 [01:15<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset augmentation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting dataset: 100%|██████████| 485/485 [01:03<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset augmentation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageEnhance\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_dataset(input_folder, output_folder, num_augmentations=5, rotation_range=(0, 360), scale_range=(0.8, 1.2), crop_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Augment dataset by rotations, scaling, cropping, and color jittering.\n",
    "\n",
    "    Parameters:\n",
    "        input_folder (str): Path to the input folder containing images.\n",
    "        output_folder (str): Path to the output folder to save augmented images.\n",
    "        num_augmentations (int): Number of augmented images to generate for each input image.\n",
    "        rotation_range (tuple): Range of rotation angles in degrees.\n",
    "        scale_range (tuple): Range of scaling factors.\n",
    "        crop_size (tuple): Size of the cropped region (height, width).\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # List all images in the input folder\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
    "\n",
    "    for image_file in tqdm(image_files, desc=\"Augmenting dataset\"):\n",
    "        # Open image\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        with Image.open(image_path) as img:\n",
    "            # Apply augmentations\n",
    "            for i in range(num_augmentations):\n",
    "                augmented_img = img.copy()\n",
    "\n",
    "                # Rotation\n",
    "                rotation_angle = random.uniform(rotation_range[0], rotation_range[1])\n",
    "                augmented_img = augmented_img.rotate(rotation_angle)\n",
    "\n",
    "                # Scaling\n",
    "                scale_factor = random.uniform(scale_range[0], scale_range[1])\n",
    "                new_size = (int(augmented_img.width * scale_factor), int(augmented_img.height * scale_factor))\n",
    "                augmented_img = augmented_img.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "                # Random cropping\n",
    "                crop_left = random.randint(0, augmented_img.width - crop_size[1])\n",
    "                crop_top = random.randint(0, augmented_img.height - crop_size[0])\n",
    "                crop_box = (crop_left, crop_top, crop_left + crop_size[1], crop_top + crop_size[0])\n",
    "                augmented_img = augmented_img.crop(crop_box)\n",
    "\n",
    "                # Color jittering\n",
    "                enhancer = ImageEnhance.Color(augmented_img)\n",
    "                enhanced_img = enhancer.enhance(random.uniform(0.5, 1.5))\n",
    "\n",
    "                # Save augmented image\n",
    "                augmented_filename = os.path.splitext(image_file)[0] + f\"_aug_{i}.png\"\n",
    "                augmented_filepath = os.path.join(output_folder, augmented_filename)\n",
    "                enhanced_img.save(augmented_filepath)\n",
    "\n",
    "    print(\"Dataset augmentation completed.\")\n",
    "\n",
    "# Example usage:\n",
    "augment_dataset(\"./datasets/dataset/trainA\", \"./datasets/augmented_dataset/trainA\")\n",
    "augment_dataset(\"./datasets/dataset/trainB\", \"./datasets/augmented_dataset/trainB\")\n",
    "set_status('augmented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-forkgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
