{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset using gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "bdd100k_url = 'https://drive.google.com/file/d/1b_WgEsT5uBwF6AypEbDsrBpKyWQu9m8p'\n",
    "lol_url = 'https://drive.google.com/file/d/1yp0T3Zjk8p7UKm4Z8XqKhYRnsSgFcppw'\n",
    "lol_synthetic url = 'https://drive.google.com/file/d/1eMj-eqe2JAh5lW5iucAO20YszlqfHNU0'\n",
    "\n",
    "gdown.download(bdd100k_url, output='bdd100k.zip', quiet=False)\n",
    "gdown.download(lol_url, output='lol.zip', quiet=False)\n",
    "gdown.download(lol_synthetic_url, output='lol_synthetic.zip', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip files to ./datasets folder\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"bdd100k.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"datasets\")\n",
    "\n",
    "# with zipfile.ZipFile('lol.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('datasets')\n",
    "\n",
    "# with zipfile.ZipFile('lol_synthetic.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/\n",
      "  trainB/\n",
      "  trainA/\n",
      "  testA/\n",
      "  testB/\n"
     ]
    }
   ],
   "source": [
    "# print the folder structure of ./datasets folder\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def print_folder_structure(folder, indent=0):\n",
    "    for item in os.listdir(folder):\n",
    "        if item.startswith(\".\"):\n",
    "            continue\n",
    "        path = os.path.join(folder, item)\n",
    "        if os.path.isdir(path):\n",
    "            print(\" \" * indent + item + \"/\")\n",
    "            print_folder_structure(path, indent + 2)\n",
    "        else:\n",
    "            # print(' ' * indent + item)\n",
    "            pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_folder_structure(\"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset folder has a file status.yaml that contains the status of the dataset\n",
    "# set the status to 'augmented' to indicate that the dataset is already augmented\n",
    "# set the status to 'original' to indicate that the dataset is the original dataset\n",
    "\n",
    "\n",
    "# function to set the status of the dataset\n",
    "def set_status(status):\n",
    "    with open(\"./datasets/dataset/status.yaml\", \"w\") as f:\n",
    "        f.write(\"status: \" + status + \"\\n\")\n",
    "\n",
    "\n",
    "# function to read the status of the dataset\n",
    "def read_status():\n",
    "    with open(\"./datasets/dataset/status.yaml\", \"r\") as f:\n",
    "        # if the file is empty, return 'original'\n",
    "        if not f.read().strip():\n",
    "            return \"original\"\n",
    "        return f.read().strip().split(\":\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting dataset:   0%|          | 0/485 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167402/2883245858.py:40: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  augmented_img = augmented_img.resize(new_size, Image.ANTIALIAS)\n",
      "Augmenting dataset: 100%|██████████| 485/485 [01:15<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset augmentation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting dataset: 100%|██████████| 485/485 [01:03<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset augmentation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageEnhance\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def augment_dataset(\n",
    "    input_folder,\n",
    "    output_folder,\n",
    "    num_augmentations=5,\n",
    "    rotation_range=(0, 360),\n",
    "    scale_range=(0.8, 1.2),\n",
    "    crop_size=(256, 256),\n",
    "):\n",
    "    \"\"\"\n",
    "    Augment dataset by rotations, scaling, cropping, and color jittering.\n",
    "\n",
    "    Parameters:\n",
    "        input_folder (str): Path to the input folder containing images.\n",
    "        output_folder (str): Path to the output folder to save augmented images.\n",
    "        num_augmentations (int): Number of augmented images to generate for each input image.\n",
    "        rotation_range (tuple): Range of rotation angles in degrees.\n",
    "        scale_range (tuple): Range of scaling factors.\n",
    "        crop_size (tuple): Size of the cropped region (height, width).\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # List all images in the input folder\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.endswith(\".png\")]\n",
    "\n",
    "    for image_file in tqdm(image_files, desc=\"Augmenting dataset\"):\n",
    "        # Open image\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        with Image.open(image_path) as img:\n",
    "            # Apply augmentations\n",
    "            for i in range(num_augmentations):\n",
    "                augmented_img = img.copy()\n",
    "\n",
    "                # Rotation\n",
    "                rotation_angle = random.uniform(rotation_range[0], rotation_range[1])\n",
    "                augmented_img = augmented_img.rotate(rotation_angle)\n",
    "\n",
    "                # Scaling\n",
    "                scale_factor = random.uniform(scale_range[0], scale_range[1])\n",
    "                new_size = (\n",
    "                    int(augmented_img.width * scale_factor),\n",
    "                    int(augmented_img.height * scale_factor),\n",
    "                )\n",
    "                augmented_img = augmented_img.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "                # Random cropping\n",
    "                crop_left = random.randint(0, augmented_img.width - crop_size[1])\n",
    "                crop_top = random.randint(0, augmented_img.height - crop_size[0])\n",
    "                crop_box = (\n",
    "                    crop_left,\n",
    "                    crop_top,\n",
    "                    crop_left + crop_size[1],\n",
    "                    crop_top + crop_size[0],\n",
    "                )\n",
    "                augmented_img = augmented_img.crop(crop_box)\n",
    "\n",
    "                # Color jittering\n",
    "                enhancer = ImageEnhance.Color(augmented_img)\n",
    "                enhanced_img = enhancer.enhance(random.uniform(0.5, 1.5))\n",
    "\n",
    "                # Save augmented image\n",
    "                augmented_filename = os.path.splitext(image_file)[0] + f\"_aug_{i}.png\"\n",
    "                augmented_filepath = os.path.join(output_folder, augmented_filename)\n",
    "                enhanced_img.save(augmented_filepath)\n",
    "\n",
    "    print(\"Dataset augmentation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "augment_dataset(\"./datasets/bdd100k/trainA\", \"./datasets/augmented_bdd100k/trainA\")\n",
    "augment_dataset(\"./datasets/bdd100k/trainB\", \"./datasets/augmented_bdd100k/trainB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-forkgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
